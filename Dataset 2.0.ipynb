{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision.transforms import RandomRotation, ToPILImage, ToTensor, ColorJitter\n",
    "import torchvision.transforms.functional as TF\n",
    "#from yolo.detect import babybox\n",
    "#from yolo.models import *\n",
    "#from yolo.utils import *\n",
    "from utils import img2uint8, pad_to_square\n",
    "tr = torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from sklearn.model_selection import KFold\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path): #Input:CH,H,W  Output:H,W,CH\n",
    "        image_pil = Image.open(str(path))\n",
    "        img = np.asarray(image_pil)\n",
    "        #img=img.transpose((1, 2, 0))  \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetDeepPhys(Dataset):\n",
    "    \"\"\"\n",
    "        Dataset class for training network.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    def __init__(self, img_paths: str, device, start=None, end=None, shift=0, crop=True, augment=False,limit=False):\n",
    "\n",
    "        # param img_paths: path to the images\n",
    "        # param device: GPU or CPU\n",
    "        \n",
    "        from utils import ReferenceProcessor #Funcion para calcular la derivada de la señal PPG\n",
    "        \n",
    "        # 1.- Parametros requeridos \n",
    "        self.H = 36\n",
    "        self.W = 36\n",
    "        self.C = 3\n",
    "        self.augment = augment\n",
    "        self.device = device\n",
    "        self.crop = crop\n",
    "        self.img_paths = img_paths\n",
    "        self.limit=limit\n",
    "        # -----------------------------\n",
    "        # Augmentation variables\n",
    "        # ----------------------------\n",
    "        self.flip_p = None\n",
    "        self.rot = None\n",
    "        self.color_transform = None\n",
    "        \n",
    "        \n",
    "        # 2.- Carga del dataset a partir de un archivo hdf5\n",
    "        \"\"\"\n",
    "        self.db_path = path\n",
    "        db = h5py.File(path, 'r')\n",
    "        frames = db['frames']\n",
    "        db_labels = db['references']\n",
    "        \"\"\"\n",
    "        \n",
    "        # Esta parte no es necesaria por que usamos un video donde ya esta casi delimitado el rostro\n",
    "        \"\"\"\"\n",
    "        # check if there is bbox\n",
    "        keys = db.keys()\n",
    "        print(keys)\n",
    "        self.is_bbox = 'bbox' in keys\n",
    "        if self.is_bbox:\n",
    "            print('\\nBounding boxes found in database, lets use them instead of YOLO!')\n",
    "            \n",
    "\n",
    "        # -----------------------------\n",
    "        # Init baby cropper\n",
    "        # -----------------------------\n",
    "        if crop and not self.is_bbox:\n",
    "            model_def = 'yolo/config/yolov3-custom.cfg'\n",
    "            weight_path = 'yolo/weights/yolov3_ckpt_42.pth'\n",
    "\n",
    "            self.yolo = Darknet(model_def).to(device)\n",
    "            self.yolo.load_state_dict(torch.load(weight_path))\n",
    "            self.yolo.eval()\n",
    "            print(\"YOLO network is initialized and ready to work!\")\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        # 3.- Carga de la señal PPG y calculo de su derivada (esta parte problemas)\n",
    "        # Create derivated ppg label\n",
    "        ppg_label = db_labels['PPGSignal']\n",
    "        refproc = ReferenceProcessor(ppg_label[shift:])\n",
    "        refproc.calculate()\n",
    "        self.label = refproc.training_label\n",
    "        \n",
    "        \n",
    "\n",
    "        (self.n, H, W, C) = frames.shape\n",
    "\n",
    "        if start is not None:\n",
    "            self.n = end - start\n",
    "            self.begin = start\n",
    "        else:\n",
    "            self.begin = 0\n",
    "\n",
    "        print(f'\\nNumber of images in the dataset: {self.n}')\n",
    "        print(f'Size of an image: {H} x {W} x {C}')\n",
    "\n",
    "        # self.num_samples = self.n - 1 - shift\n",
    "        # To be in line with physnet\n",
    "        \n",
    "        \n",
    "        \n",
    "        # NO entiendo bien\n",
    "        tmp = ((self.n - 64) // 128) \n",
    "        self.num_samples = tmp * 128 - 1 - shift\n",
    "        db.close()\n",
    "        \"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # Data augmentation en caso sea necesario\n",
    "        if self.augment:\n",
    "            # Set up the same image transforms for the chunk\n",
    "            self.flip_p = random.random()\n",
    "            self.hflip_p = random.random()\n",
    "            # self.rot = RandomRotation.get_params((0, 90))\n",
    "            self.color_transform = ColorJitter.get_params(brightness=(0.3, 1.5),\n",
    "                                                          contrast=(0.8, 1.2),\n",
    "                                                          saturation=(0.8, 1.2),\n",
    "                                                          hue=(0, 0))\n",
    "\n",
    "            \n",
    "        # Target construido a partir de la señal PPG derivada\n",
    "        # Construct target signals\n",
    "        #arget = tr.tensor(self.label[idx])\n",
    "        \n",
    "        \n",
    "\n",
    "        # Construct networks input\n",
    "        A = tr.empty(self.C, self.H, self.W, dtype=tr.float)\n",
    "        M = tr.empty(self.C, self.H, self.W, dtype=tr.float)\n",
    "        \n",
    "        \n",
    "        #4.1.- Carga del dataset a partir de hdf5 files\n",
    "        \"\"\"\n",
    "        with h5py.File(self.db_path, 'r') as db:\n",
    "            frames = db['frames']\n",
    "            img1 = frames[idx, :, :, :]\n",
    "            img2 = frames[idx + 1, :, :, :]\n",
    "\n",
    "            # ----------------------------\n",
    "            # Crop baby with yolo\n",
    "            # ----------------------------\n",
    "            if self.crop and self.is_bbox:\n",
    "                bbox = db['bbox'][idx, :]\n",
    "                y1, y2, x1, x2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "\n",
    "                # check to be inside image size\n",
    "                if y2 > img1.shape[0]:\n",
    "                    y2 = img1.shape[0]\n",
    "                if x2 > img1.shape[1]:\n",
    "                    x2 = img1.shape[1]\n",
    "                if y1 < 0:\n",
    "                    y1 = 0\n",
    "                if x1 < 0:\n",
    "                    x1 = 0\n",
    "                # check validity\n",
    "                if y2-y1 < 1 or x2-x1 < 1:\n",
    "                    y1 = x1 = 0\n",
    "                    y2, x2 = img1.shape[:2]\n",
    "\n",
    "                img1 = img1[y1:y2, x1:x2, :]\n",
    "                img2 = img2[y1:y2, x1:x2, :]\n",
    "            elif self.crop and not self.is_bbox:\n",
    "                x1, y1, x2, y2 = babybox(self.yolo, img1, self.device)\n",
    "                img1 = img1[y1:y2, x1:x2, :]\n",
    "                img2 = img2[y1:y2, x1:x2, :]     \n",
    "        \"\"\"      \n",
    "        #4.2- Carga del dataset a partir de img files (mantener transformaciones)\n",
    "        \n",
    "        if self.limit is None:\n",
    "            img_file_name_1 = self.img_paths[idx]\n",
    "            img_file_name_2 = self.img_paths[idx+1]\n",
    "        else:\n",
    "            img_file_name_1 = np.random.choice(self.img_paths)\n",
    "            img_file_name_2 = np.random.choice(self.img_paths)\n",
    "        \n",
    "        img1 = load_image(img_file_name_1)\n",
    "        img2 = load_image(img_file_name_2)\n",
    "        \n",
    "\n",
    "        # Downsample image\n",
    "        try:\n",
    "            img1 = cv2.resize(img1, (self.H, self.W), interpolation=cv2.INTER_CUBIC)\n",
    "            img2 = cv2.resize(img2, (self.H, self.W), interpolation=cv2.INTER_CUBIC)\n",
    "        except:\n",
    "            print('\\n--------- ERROR! -----------\\nUsual cv empty error')\n",
    "            print(f'Shape of img1: {img1.shape}; Shape of im2: {img2.shape}')\n",
    "            print(f'bbox: {bbox}')\n",
    "            print(f'This is at idx: {idx}')\n",
    "            exit(666)\n",
    "\n",
    "        if self.augment:\n",
    "            img1 = ToPILImage()(img1)\n",
    "            img2 = ToPILImage()(img2)\n",
    "            if self.flip_p > 0.5:\n",
    "                img1 = TF.vflip(img1)\n",
    "                img2 = TF.vflip(img2)\n",
    "            if self.flip_p > 0.5:\n",
    "                img1 = TF.hflip(img1)\n",
    "                img2 = TF.hflip(img2)\n",
    "\n",
    "            # img1 = TF.rotate(img1, self.rot)\n",
    "            # img2 = TF.rotate(img2, self.rot)\n",
    "\n",
    "            img1 = self.color_transform(img1)\n",
    "            img2 = self.color_transform(img2)\n",
    "\n",
    "            img1 = tr.from_numpy(np.array(img1).astype(np.float32))\n",
    "            img2 = tr.from_numpy(np.array(img2).astype(np.float32))\n",
    "            img1 = img1.permute(2, 0, 1)\n",
    "            img2 = img2.permute(2, 0, 1)\n",
    "        else:\n",
    "            img1 = tr.from_numpy(img1.astype(np.float32))\n",
    "            img2 = tr.from_numpy(img2.astype(np.float32))\n",
    "            # Swap axes because  numpy image: H x W x C | torch image: C X H X W\n",
    "            img1 = img1.permute(2, 0, 1)\n",
    "            img2 = img2.permute(2, 0, 1)\n",
    "\n",
    "\n",
    "    \n",
    "        # 2.) construct the normalized frame difference for motion stream input\n",
    "        M = tr.div(img2 - img1, img1 + img2 + 1)         # +1 for numerical stability\n",
    "        # M = tr.sub(M, tr.mean(M, (1, 2)).view(3, 1, 1))  # spatial intensity norm for each channel\n",
    "\n",
    "        A = img1/255.  # convert image to [0, 1]\n",
    "        print(A.shape)\n",
    "        A = tr.sub(A, tr.mean(A, (1, 2)).view(3, 1, 1))  # spatial intensity norm for each channel\n",
    "\n",
    "        #sample = ((A, M), target)\n",
    "        sample = (A,M)\n",
    "\n",
    "        # Video shape: C x D x H X W\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path='C:/Users/Ramfis/Desktop/Universidad/dataset 2/VIPL-HR-V1/VIPL-HR-V1/data/p1/v1/source1/frames'\n",
    "train_val_file_names=np.array(sorted(glob.glob(str(data_path)+ \"/*.jpg\")))\n",
    "test_file_names =  np.array(sorted(glob.glob(str(data_path) + \"/*.jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C:/Users/Ramfis/Desktop/Universidad/dataset 2/VIPL-HR-V1/VIPL-HR-V1/data/p1/v1/source1/frames\\\\frame1.jpg',\n",
       "       'C:/Users/Ramfis/Desktop/Universidad/dataset 2/VIPL-HR-V1/VIPL-HR-V1/data/p1/v1/source1/frames\\\\frame10.jpg',\n",
       "       'C:/Users/Ramfis/Desktop/Universidad/dataset 2/VIPL-HR-V1/VIPL-HR-V1/data/p1/v1/source1/frames\\\\frame100.jpg',\n",
       "       ...,\n",
       "       'C:/Users/Ramfis/Desktop/Universidad/dataset 2/VIPL-HR-V1/VIPL-HR-V1/data/p1/v1/source1/frames\\\\frame997.jpg',\n",
       "       'C:/Users/Ramfis/Desktop/Universidad/dataset 2/VIPL-HR-V1/VIPL-HR-V1/data/p1/v1/source1/frames\\\\frame998.jpg',\n",
       "       'C:/Users/Ramfis/Desktop/Universidad/dataset 2/VIPL-HR-V1/VIPL-HR-V1/data/p1/v1/source1/frames\\\\frame999.jpg'],\n",
       "      dtype='<U107')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_in(train_file_names, fold, num_splits=5):\n",
    " \n",
    "    kf = KFold(n_splits=num_splits, random_state=2019,shuffle=True)\n",
    "    #kf = KFold(n_splits=num_splits, random_state=20018)\n",
    "\n",
    "\n",
    "    ids = list(kf.split(train_file_names))\n",
    "\n",
    "    train_ids, val_ids = ids[fold]\n",
    "\n",
    "    if fold == -1:\n",
    "        return train_file_names, train_file_names\n",
    "    else:\n",
    "        return train_file_names[train_ids], train_file_names[val_ids]\n",
    "    \n",
    "def percent_split(train_val_100percent, percent = 1): \n",
    "    \n",
    "    fpath_list = train_val_100percent\n",
    "\n",
    "\n",
    "    dataset_size = len(fpath_list)\n",
    "    indices = list(range(dataset_size))\n",
    "    percent = int(np.floor(percent * dataset_size))\n",
    "    if 1 :\n",
    "        np.random.seed(2019)\n",
    "        np.random.shuffle(indices)        \n",
    "\n",
    "    extra_indices, train_indices_split = indices[percent:], indices[:percent]\n",
    "    print(dataset_size,len(train_indices_split), len(extra_indices))\n",
    "    \n",
    "   \n",
    "    return train_val_100percent[extra_indices],train_val_100percent[train_indices_split] #\n",
    "\n",
    "def get_split_out(data_path,name_file, fold, num_splits=5):\n",
    "    #data_path=Path(path)\n",
    "\n",
    "    train_path = data_path / name_file / 'images'\n",
    "\n",
    "    train_file_names = np.array(sorted(list(train_path.glob('*npy'))))\n",
    "\n",
    "    kf = KFold(n_splits=num_splits, random_state=2019,shuffle=True)\n",
    "    \n",
    "\n",
    "\n",
    "    ids = list(kf.split(train_file_names))\n",
    "\n",
    "    train_ids, val_ids = ids[fold]\n",
    "\n",
    "    if fold == -1:\n",
    "        return train_file_names, train_file_names\n",
    "    else:\n",
    "        return train_file_names[train_ids], train_file_names[val_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_names,val_file_names = get_split_in(train_val_file_names,-1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C:/Users/Ramfis/Desktop/Universidad/dataset 2/VIPL-HR-V1/VIPL-HR-V1/data/p1/v1/source1/frames\\\\frame1.jpg',\n",
       "       'C:/Users/Ramfis/Desktop/Universidad/dataset 2/VIPL-HR-V1/VIPL-HR-V1/data/p1/v1/source1/frames\\\\frame10.jpg',\n",
       "       'C:/Users/Ramfis/Desktop/Universidad/dataset 2/VIPL-HR-V1/VIPL-HR-V1/data/p1/v1/source1/frames\\\\frame100.jpg',\n",
       "       ...,\n",
       "       'C:/Users/Ramfis/Desktop/Universidad/dataset 2/VIPL-HR-V1/VIPL-HR-V1/data/p1/v1/source1/frames\\\\frame997.jpg',\n",
       "       'C:/Users/Ramfis/Desktop/Universidad/dataset 2/VIPL-HR-V1/VIPL-HR-V1/data/p1/v1/source1/frames\\\\frame998.jpg',\n",
       "       'C:/Users/Ramfis/Desktop/Universidad/dataset 2/VIPL-HR-V1/VIPL-HR-V1/data/p1/v1/source1/frames\\\\frame999.jpg'],\n",
       "      dtype='<U107')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepphys_dset_test(idx):\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    dset = DatasetDeepPhys(train_file_names, device)\n",
    "    A, M = dset[idx]\n",
    "\n",
    "    #print('target: ', point)\n",
    "\n",
    "    A = A.permute(1, 2, 0).data.cpu().numpy()\n",
    "    M = M.permute(1, 2, 0).data.cpu().numpy()\n",
    "\n",
    "    cv2.namedWindow('A', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('A', 500, 500)\n",
    "    A = convert2cvimshow(A)\n",
    "    cv2.imshow('A', A)\n",
    "\n",
    "    cv2.namedWindow('M', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('M', 500, 500)\n",
    "    M = convert2cvimshow(M)\n",
    "    cv2.imshow('M', M)\n",
    "\n",
    "    while cv2.waitKey(1) & 0xFF != ord('q'):\n",
    "        pass\n",
    "\n",
    "    # Release everything if job is finished\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2cvimshow(img):\n",
    "    frame = img.copy()\n",
    "    frame = frame - np.min(frame)\n",
    "    frame = frame / np.max(frame)\n",
    "    frame = (frame * 255).astype(np.uint8)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "deepphys_dset_test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
